{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c41166",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./generate_replies_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d85695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|start|>', eos_token='<|end|>', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        '<|start|>'+ examples[\"op_with_reply_text\"] + '<|end|>',\n",
    "        truncation=True,\n",
    "        max_length= 583 # Tweet max = 280, 2 tweets + \"{REPLY}\" + start and end of text tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'final_liked_gpt.csv'\n",
    "test_path = 'final_liked_gpt_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a511d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"final_liked_gpt.csv\", \"test\": \"final_liked_gpt_test.csv\"}\n",
    "dataset_base = load_dataset(\"csv\", data_files=data_files)\n",
    "dataset_base = dataset_base.map(tokenize_and_split, remove_columns=['op_id','reply_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af24b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_base[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "dataset[\"validation\"] = dataset.pop(\"test\") # Renames the default feature \"test\" split to \"validation\"\n",
    "dataset[\"test\"] = dataset_base[\"test\"] # Add the \"test\" feature\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa991b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELOAD MODEL\n",
    "\n",
    "#configuration = GPT2Config.from_pretrained(model_dir, output_hidden_states=False)\n",
    "#model = GPT2LMHeadModel.from_pretrained(model_dir, config=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22500c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d205b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda() # Run on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=5, \n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps = 1500, \n",
    "    save_steps=1500,\n",
    "    warmup_steps=500,\n",
    "    evaluation_strategy=\"steps\"\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2eab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265015b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd00f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs(input_text, nb_seq):\n",
    "    text_to_generate = \"<|start|>\" + input_text + \"{REPLY}\"\n",
    "    \n",
    "    generated_output = torch.tensor(tokenizer.encode(input_text)).unsqueeze(0).to(device)\n",
    "    outputs = model.generate(\n",
    "            generated_output, \n",
    "            do_sample=True,   \n",
    "            top_k=50, \n",
    "            max_length = 567,\n",
    "            top_p=0.95, \n",
    "            num_return_sequences=nb_seq\n",
    "        )\n",
    "    return [tokenizer.decode(o, skip_special_tokens=True).split('{REPLY}')[1] for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_to_reply_to = \"What are you going to do for the holidays?\" # Put a tweet to reply to here\n",
    "\n",
    "decoded_outputs = generate_outputs(tweet_to_reply_to, 3)\n",
    "\n",
    "for i, output in enumerate(decoded_outputs):\n",
    "    if len(output) > 1:\n",
    "        print(\"{}: {}\\n\\n\".format(i, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7da6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffcafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.push_to_hub(\"twitter_reply_generator\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
